# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NaeBo_Io6w4JFp1fy12MtxipUOd0LGV8
"""

!pip install -U keras
!pip install -U  datasets
!pip install -U  tensorflow

"""# load the rquired modules"""

from transformers import AutoTokenizer , TFAutoModelForSequenceClassification
from datasets import Dataset
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

"""# load and text the model i will use to make classification"""

model_id = "albert/albert-base-v2"

tokenizer =  AutoTokenizer.from_pretrained(model_id)
model =  TFAutoModelForSequenceClassification.from_pretrained(model_id,num_labels=6)
#tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no pad token

"""# load the train data and prepare it"""

import pandas as pd

splits = {'train': 'split/train-00000-of-00001.parquet',
          'validation': 'split/validation-00000-of-00001.parquet',
          'test': 'split/test-00000-of-00001.parquet'}

df_train = pd.read_parquet("hf://datasets/dair-ai/emotion/" + splits["train"])
df_val = pd.read_parquet("hf://datasets/dair-ai/emotion/" + splits["validation"])
df_test = pd.read_parquet("hf://datasets/dair-ai/emotion/" + splits["test"])

#the model input is input_ids and attention_mask , and the target is label
#for more methods and control conver data into dataset of hugging face

ds_train = Dataset.from_pandas(df_train)
ds_val =  Dataset.from_pandas(df_val)
ds_test = Dataset.from_pandas(df_test)

del df_train, df_val, df_test

#make dataset from generator

def t_generator():
  for sample in ds_train:
    tokens = tokenizer(sample['text'] ,truncation=True , padding='max_length', max_length=100,return_tensors='tf')
    yield(
    {
        'input_ids':tokens['input_ids'][0],
        'attention_mask':tokens['attention_mask'][0]
    },sample['label'])

def v_generator():
  for sample in ds_val:
    tokens = tokenizer(sample['text'] ,truncation=True , padding='max_length', max_length=100,return_tensors='tf')
    yield (
    {
        'input_ids':tokens['input_ids'][0],
        'attention_mask':tokens['attention_mask'][0]
    },sample['label'])


# Output signature for tf.data.Dataset
output_signature = (
    {
        "input_ids": tf.TensorSpec(shape=(100,), dtype=tf.int32),
        "attention_mask": tf.TensorSpec(shape=(100,), dtype=tf.int32),
    },
    tf.TensorSpec(shape=(), dtype=tf.int64)
)

train_dataset = tf.data.Dataset.from_generator(t_generator, output_signature=output_signature).shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)
val_dataset = tf.data.Dataset.from_generator(v_generator, output_signature=output_signature).batch(32).prefetch(tf.data.AUTOTUNE)

# âœ… Now ready for training

"""#train the model"""

#choose layers to freez and layers to train


# make everything trainable
for layer in model.layers:
    layer.trainable = True



# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"]#metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
)



ES = tf.keras.callbacks.EarlyStopping(
     monitor="val_loss",  # or "val_accuracy"
    patience=2,
    restore_best_weights=True

)

# Train model
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=100,
    callbacks=[ES]

)

# Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Model Accuracy")
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Model Loss")
plt.legend()

plt.tight_layout()
plt.show()

model.save_pretrained("classiv1_albert_model")
tokenizer.save_pretrained("classiv1_albert_model")